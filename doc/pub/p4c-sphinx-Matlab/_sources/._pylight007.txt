

.. |nbsp| unicode:: 0xA0
   :trim:

.. !split

Solving nonlinear algebraic equations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

.. figure:: comic_root.png
   :width: 800

As a reader of this book you are probably well into mathematics and
often "accused" of being particularly good at "solving equations" (typically
at family dinners!). However, is it *really* true that you, with pen and paper, can
solve *many* equations? How many kinds of equations can you actually solve?
If we restrict our attention to *algebraic equations in one unknown* :math:`x`,
you can certainly do linear equations: :math:`ax+b=0` and quadratic
ones: :math:`ax^2 + bx + c = 0`. You may also know that there
are formulas for the roots of cubic and quartic equations too.
Maybe you can do the special trigonometric equation
:math:`\sin x + \cos x = 1` as well, but there it (probably) stops. Equations that are
not reducible to one of the mentioned cannot be solved, which
means that most algebraic equations arising in applications cannot
be treated with pen and paper!

If we exchange the traditional idea of finding *exact* solutions to equations with
the idea of rather finding *approximate* solutions, a whole new world of possibilities
opens up! With such an approach, we can in principle solve *any* algebraic equation.

Let us start by introducing a common generic form for any algebraic equation:

.. math::
         f(x) = 0\thinspace .

Here, :math:`f(x)` is some prescribed formula involving :math:`x`. For example,
the equation

.. math::
         e^{-x}\sin x = \cos x

has

.. math::
         f(x)= e^{-x}\sin x - \cos x\thinspace .

Just move all terms to the left-hand side and then the formula
to the left of the equality sign is :math:`f(x)`.

So, when do we really need to solve algebraic equations beyond the
simplest types we can treat with pen and paper?  There are two major
application areas. One is when using *implicit* numerical methods for
ordinary differential equations.  These give rise to one or a system of
algebraic equations.  The other major application type is
optimization, i.e., finding the maxima or minima of a function.  These
maxima and minima are normally found by solving the algebraic equation
:math:`F'(x)=0` if :math:`F(x)` is the function to be optimized.  Differential
equations are very much used throughout science and engineering, and
actually most engineering problems are optimization problems in the
end because one wants a design that maximizes performance and
minimizes cost.

We restrict the attention here to one algebraic equation in one variable,
with our usual emphasis how to program the algorithms.
*Systems* of nonlinear algebraic equations with *many variables*
arise from implicit methods for ordinary and partial differential
equations as well as in multivariate optimization. However,
we consider this topic beyond the scope of the current text.

.. index:: root finding


.. admonition:: Terminology

   When solving algebraic equations :math:`f(x)=0`, we often say that the
   solution :math:`x` is a *root* of the equation. The solution process itself
   is thus often called *root finding*.




.. _4th:bruteforce:

Brute force methods
===================

The representation of a mathematical function :math:`f(x)` on a computer
takes two forms.  One is a Matlab function returning the function
value given the argument, while the other is a collection of points :math:`(x,
f(x))` along the function curve. The latter is the representation we
use for plotting, together with an assumption of linear variation
between the points. This representation is also very suited for
equation solving and optimization: we simply go through all points and
see if the function crosses the :math:`x` axis, or for optimization, test
for a local maximum or minimum point. Because there is a lot of work to
examine a huge number of points, and also because the idea is extremely
simple, such approaches are often referred to as *brute force*
methods. However, we are not embarrassed of explaining the methods
in detail and implementing them.

Brute force root finding
------------------------

Assume that we have a set of points along the curve of a function :math:`f(x)`:

.. figure:: brute_force_roots.png
   :width: 600

We want to solve :math:`f(x)=0`, i.e., find the points :math:`x`
where :math:`f` crosses the :math:`x` axis.
A brute force algorithm is to run through all points on the
curve and check if one point is below the :math:`x` axis and if the next
point is above the :math:`x` axis, or the other way around.
If this is found to be the case, we know that :math:`f` must be zero
in between these two :math:`x` points.

Numerical algorithm  (1)
~~~~~~~~~~~~~~~~~~~~~~~~

More precisely, we have a set of :math:`n+1` points :math:`(x_i, y_i))`,
:math:`y_i=f(x_i)`, :math:`i=0,\ldots,n`, where
:math:`x_0 < \ldots < x_n`.
We check if :math:`y_i < 0` and :math:`y_{i+1} > 0` (or the other way around). A compact expression
for this check is to perform the test :math:`y_i y_{i+1} < 0`.
If so, the root of :math:`f(x)=0`
is in :math:`[x_i, x_{i+1}]`. Assuming a linear variation of :math:`f`
between :math:`x_i` and :math:`x_{i+1}`, we have the approximation

.. math::
        
        f(x)\approx \frac{f(x_{i+1})-f(x_i)}{x_{i+1}-x_i}(x-x_i) + f(x_i)
        = \frac{y_{i+1}-y_i}{x_{i+1}-x_i}(x-x_i) + y_i,
        

which, when set equal to zero, gives the root

.. math::
         x = x_i - \frac{x_{i+1}-x_i}{y_{i+1}-y_i}y_i\thinspace .

Implementation  (6)
~~~~~~~~~~~~~~~~~~~

Given some Matlab implementation ``f(x)`` of our mathematical
function, straightforward programming of the above numerical
algorithm leads to

.. code-block:: matlab

        x = linspace(0, 4, 10001);
        y = f(x);
        
        root = NaN;  % Initialization
        for i = 1:(length(x)-1) 
            if y(i)*y(i+1) < 0
                 root = x(i) - (x(i+1) - x(i))/(y(i+1) - y(i))*y(i);
                 break;  % Jump out of loop
            end
        end
        if isnan(root)
            fprintf('Could not find any root in [%g, %g]\n', x(0), x(-1));
        else
            fprintf('Find (the first) root as x=%g\n', root);
        end

(See the file
`brute_force_root_finder_flat.m <https://github.com/hplgit/prog4comp/src/m/brute_force_root_finder_flat.m>`__.)

Note the nice use of setting ``root`` to ``NaN``: we can simply test
``if isnan(root)`` to see if we found a root and overwrote the ``NaN``
value, or if we did not find any root among the tested points.

Running this program with some function, say :math:`f(x)=e^{-x^2}\cos(4x)`
(which has a solution at :math:`x = \frac{\pi}{8}`),
gives the root 0.392699, which has an error of :math:`8.2\cdot 10^{-8}`.
Increasing the number of points with a factor of ten gives a root
with an error of :math:`3.1\cdot 10^{-10}`.

After such a quick "flat" implementation of an algorithm, we should
always try to offer the algorithm as a Matlab function, applicable to as
wide a problem domain as possible. The function should take :math:`f` and
an associated interval
:math:`[a,b]` as input, as well as a number of points (:math:`n`), and return
a list of all the roots in :math:`[a,b]`. Here is our candidate
for a good implementation of the brute force rooting finding algorithm:

.. code-block:: matlab

        function all_roots = brute_force_root_finder(f, a, b, n)
            x = linspace(a, b, n);
            y = f(x);
            roots = [];
            for i = 1:(n-1)
                if y(i)*y(i+1) < 0
                    root = x(i) - (x(i+1) - x(i))/(y(i+1) - y(i))*y(i);
                    roots = [roots; root];
                end
            end
            all_roots = roots;
        end

This function is found in the file
`brute_force_root_finder.m <https://github.com/hplgit/prog4comp/src/m/brute_force_foot_finder.m>`__.

This time we use another elegant technique to indicate if roots were found
or not: ``roots`` is empty (an array of length zero) if the root finding was unsuccessful,
otherwise it contains all the roots. Application of the function to
the previous example can be coded as
(`demo_brute_force_root_finder.m <https://github.com/hplgit/prog4comp/src/m/demo_brute_force_foot_finder.m>`__):

.. code-block:: matlab

        function demo_brute_force_root_finder()
            roots = brute_force_root_finder(
                @(x) exp(-x.^2).*cos(4*x), 0, 4, 1001);
            if length(roots) > 0
                roots
            else
                fprintf('Could not find any roots');
            end
        end

Brute force optimization
------------------------

Numerical algorithm  (2)
~~~~~~~~~~~~~~~~~~~~~~~~

We realize that :math:`x_i`
corresponds to a maximum point if :math:`y_{i-1} < y_i > y_{i+1}`.
Similarly, :math:`x_i` corresponds to a minimum if
:math:`y_{i-1} > y_i < y_{i+1}`. We can do this test for all "inner"
points :math:`i=1,\ldots,n-1` to find all local minima and maxima.
In addition, we need to add an end
point, :math:`i=0` or :math:`i=n`, if the corresponding :math:`y_i` is a global
maximum or minimum.

Implementation  (7)
~~~~~~~~~~~~~~~~~~~

The algorithm above can be translated to the following Matlab function
(file `brute_force_optimizer.m <https://github.com/hplgit/prog4comp/src/m/brute_force_optimizer.m>`__):

.. code-block:: matlab

        function [xy_minima, xy_maxima] = brute_force_optimizer(f, a, b, n)
            x = linspace(a, b, n);
            y = f(x);
            % Let maxima and minima hold the indices corresponding
            % to (local) maxima and minima points    
            minima = [];
            maxima = [];
            for i = 2:(n-1)
                if y(i-1) < y(i) && y(i) > y(i+1)
                    maxima = [maxima; i];
                end
                if y(i-1) > y(i) && y(i) < y(i+1)
                    minima = [minima; i];
                end
            end
        
            % What about the end points?
            y_min_inner = y(minima(1));   % Initialize
            for i = 1:length(minima)
                if y(minima(i)) < y_min_inner
                    y_min_inner = y(minima(i));
                end        
            end
            y_max_inner = y(maxima(1));   % Initialize
            for i = 1:length(maxima)
                if y(maxima(i)) > y_max_inner
                    y_max_inner = y(maxima(i));
                end
            end
            
            if y(1) > y_max_inner
                maxima = [maxima; 1];
            end
            if y(length(x)) > y_max_inner
               maxima = [maxima; length(x)];
            end
            if y(1) < y_min_inner
                minima = [minima; 1];
            end
            if y(length(x)) < y_min_inner
                minima = [minima; length(x)];
            end
            
            % Compose return values
            xy_minima = [];
            for i = 1:length(minima)
                xy_minima = [xy_minima; [x(minima(i)) y(minima(i))]];
            end 
            xy_maxima = [];
            for i = 1:length(maxima)
                xy_maxima = [xy_maxima; [x(maxima(i)) y(maxima(i))]];
            end 
        end

An application to :math:`f(x)=e^{-x^2}\cos(4x)` looks like

.. code-block:: matlab

        function demo_brute_force_optimizer
            [xy_minima, xy_maxima] = brute_force_optimizer(
                @(x) exp(-x.^2).*cos(4*x), 0, 4, 1001);
            xy_minima
            xy_maxima
        end

Model problem for algebraic equations
-------------------------------------

We shall consider the
very simple problem of finding the square root of 9, which is the
positive solution of :math:`x^2=9`. The nice feature of solving an equation
whose solution is known beforehand is that we can easier investigate
how the numerical method and the implementation perform in the search
for the solution. The :math:`f(x)` function corresponding to the equation
:math:`x^2=9` is

.. math::
         f(x) = x^2 - 9\thinspace .

Our interval of interest for solutions will be :math:`[0,1000]` (the upper
limit here is chosen somewhat arbitrarily).

In the following, we will present several efficient and
accurate methods for solving nonlinear algebraic equations, both
single equations and systems of equations. The methods all have in
common that they search for *approximate* solutions.
The methods differ, however, in the
way they perform the search for solutions. The idea for the search
influences the efficiency of the search and the reliability of
actually finding a solution. For example, Newton's method is very
fast, but not reliable, while the bisection method is the slowest,
but absolutely reliable. No method is best at all problems, so we
need different methods for different problems.


.. admonition:: What is the difference between linear and nonlinear equations

   You know how to solve linear equations :math:`ax+b=0`: :math:`x=-b/a`.
   All other types of equations :math:`f(x)`, i.e., when :math:`f(x)` is not a linear
   function of :math:`x`, are called nonlinear.
   A typical way of recognizing a nonlinear equation is to observe
   that :math:`x` is "not alone" as in :math:`ax`,
   but involved in a product with itself, such as in :math:`x^3 + 2x^2 -9=0`.
   We say that :math:`x^3` and :math:`2x^2` are nonlinear terms. An equation like
   :math:`\sin x + e^x\cos x=0` is also nonlinear although :math:`x` is not explicitly
   multiplied by itself, but the Taylor series of :math:`\sin x`, :math:`e^x`, and
   :math:`\cos x` all involve polynomials of :math:`x` where :math:`x` is multiplied by itself.




.. _4th:NonlinAlgEq:Newt:

Newton's method
===============

*Newton's method*, also known as *Newton-Raphson's method*, is a very
famous and widely used method for solving nonlinear algebraic
equations. Compared to the other methods we will consider, it is
generally the fastest one (usually by far). It does not guarantee
that an existing solution will be found, however.

A fundamental idea of numerical methods for nonlinear
equations have been to construct a series of linear equations, since we
can solve linear equations, such that solving the linear equations brings
us closer and closer to the solution of the nonlinear equation.
The idea will be clearer when we present Newton's method and the secant method.

Deriving and implementing Newton's method
-----------------------------------------

Figure :ref:`fig:illustrNewton` shows the :math:`f(x)` function in
our model equation :math:`x^2-9=0`. Numerical methods for algebraic equations
require us to guess at a solution first. Here, this guess is called
:math:`x_0`. The fundamental idea of Newton's method is to approximate
the original function :math:`f(x)` by a straight line, i.e., a linear
function, since it is straightforward to solve linear equations.
There are infinitely many choices of how to approximate :math:`f(x)` by
a straight line. Newton's method applies the tangent of :math:`f(x)` at
:math:`x_0`, see the rightmost tangent in Figure :ref:`fig:illustrNewton`.
This linear tangent function crosses the :math:`x` axis at a point we
call :math:`x_1`. This is (hopefully) a better approximation to the solution
of :math:`f(x)=0` than :math:`x_0`. The next fundamental idea is to repeat
this process. We find the tangent of :math:`f` at :math:`x_1`, compute where it
crosses the :math:`x` axis, at a point called :math:`x_2`, and repeat the process
again. Figure :ref:`fig:illustrNewton` shows that the process brings
us closer and closer to the left. It remains, however, to see if
we hit :math:`x=3` or come sufficiently close to this solution.

.. _fig:illustrNewton:

.. figure:: fig4_4.png
   :width: 400

   *Illustrates the idea of Newton's method with :math:`f(x) = x^2 - 9`, repeatedly solving for crossing of tangent lines with the :math:`x` axis*

How do we compute the tangent of a function :math:`f(x)` at a point :math:`x_0`?
The tangent function, here called :math:`\tilde f(x)`, is linear and has two
properties:

1. the slope equals to :math:`f'(x_0)`

2. the tangent touches the :math:`f(x)` curve at :math:`x_0`

So, if we write the tangent function as :math:`\tilde f(x)=ax+b`, we must
require :math:`\tilde f'(x_0)=f'(x_0)` and :math:`\tilde f(x_0)=f(x_0)`, resulting in

.. math::
         \tilde f(x) = f(x_0) + f'(x_0)(x - x_0)\thinspace . 

The key step in Newton's method is to find where the tangent crosses
the :math:`x` axis, which means solving :math:`\tilde f(x)=0`:

.. math::
         \tilde f(x)=0\quad\Rightarrow\quad x = x_0 - \frac{f(x_0)}{f'(x_0)}
        \thinspace .

This is our new candidate point, which we call :math:`x_1`:

.. math::
         x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}\thinspace . 

With :math:`x_0 = 1000`, we get :math:`x_1 \approx 500`, which is in accordance
with the graph in Figure :ref:`fig:illustrNewton`. Repeating the
process, we get

.. math::
         x_2 = x_1 - \frac{f(x_1)}{f'(x_1)}\approx 250\thinspace . 

The general scheme of Newton's method may be written as

.. _Eq:eFourEleven:

.. math::
   :label: eFourEleven
        
        x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)},\quad n=0,1,2,\ldots
        
        

The computation in :eq:`eFourEleven` is repeated until
:math:`f\left(x_n\right)` is close enough to zero. More precisely,
we test if :math:`|f(x_n)|<\epsilon`, with :math:`\epsilon` being a small
number.

We moved from 1000 to 250 in two iterations, so it is exciting to see
how fast we can approach the solution :math:`x=3`.
A computer program can automate the calculations. Our first
try at implementing Newton's method is in a function ``naive_Newton``:

.. code-block:: matlab

        function result = naive_Newton(f,dfdx,starting_value,eps)
            x = starting_value;
            while abs(f(x)) > eps
                x = x - f(x)/dfdx(x);
            end
            result = x;
        end

The argument ``x`` is the starting value, called :math:`x_0` in our previous
description.
To solve the problem :math:`x^2=9` we also need to implement

.. code-block:: matlab

        function result = f(x)
            result = x^2 - 9;
        end
        
        function result = dfdx(x)
            result = 2*x;
        end


.. admonition:: Why not use an array for the :math:`x` approximations

   Newton's method is normally formulated with an *iteration index* :math:`n`,
   
   .. math::
            x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\thinspace .
   
   Seeing such an index, many would implement this as
   
   .. code-block:: matlab
   
           x(n+1) = x(x) - f(x(n))/dfdx(x(n));
   
   Such an array is fine, but requires storage of all the approximations.
   In large industrial applications, where Newton's method solves
   millions of equations at once, one cannot afford to store all the
   intermediate approximations in memory, so then it is important to
   understand that the algorithm in Newton's method has no more need
   for :math:`x_n` when :math:`x_{n+1}` is computed. Therefore, we can work with
   one variable ``x`` and overwrite the previous value:
   
   .. code-block:: python
   
           x = x - f(x)/dfdx(x)




Running ``naive_Newton(f, dfdx, 1000, eps=0.001)`` results in the approximate
solution 3.000027639. A smaller value of ``eps`` will produce a more
accurate solution. Unfortunately, the plain ``naive_Newton`` function
does not return how many iterations it used, nor does it print out
all the approximations :math:`x_0,x_1,x_2,\ldots`, which would indeed be a nice
feature. If we insert such a printout, a rerun results in

.. code-block:: text

        500.0045
        250.011249919
        125.02362415
        62.5478052723
        31.3458476066
        15.816483488
        8.1927550496
        4.64564330569
        3.2914711388
        3.01290538807
        3.00002763928

We clearly see that the iterations approach the solution quickly.
This speed of the search for the solution is the primary strength of
Newton's method compared to other methods.

.. _4th:Newton:robust:

Making a more efficient and robust implementation
-------------------------------------------------

.. index::
   single: code; robust

The ``naive_Newton`` function works fine for the example we are considering
here. However, for more general use, there are some pitfalls that
should be fixed in an improved version of the code. An example
may illustrate what the problem is: let us solve :math:`\tanh(x)=0`, which
has solution :math:`x=0`. With :math:`|x_0|\leq 1.08` everything works fine. For
example, :math:`x_0` leads to six iterations if :math:`\epsilon=0.001`:

.. code-block:: text

        -1.05895313436
        0.989404207298
        -0.784566773086
        0.36399816111
        -0.0330146961372
        2.3995252668e-05

Adjusting :math:`x_0` slightly to 1.09 gives division by zero! The
approximations computed by Newton's method become

.. code-block:: text

        -1.09331618202
        1.10490354324
        -1.14615550788
        1.30303261823
        -2.06492300238
        13.4731428006
        -1.26055913647e+11

The division by zero is caused by :math:`x_7=-1.26055913647\cdot 10^{11}`,
because :math:`\tanh(x_7)` is 1.0 to machine precision, and then
:math:`f'(x)=1 - \tanh(x)^2` becomes zero in the denominator in Newton's
method.

The underlying problem, leading to the division by zero in the above
example, is that Newton's method *diverges*: the approximations move
further and further away from :math:`x=0`. If it had not been for the
division by zero, the condition in the ``while`` loop would always
be true and the loop would run forever. Divergence of Newton's method
occasionally happens, and the remedy
is to abort the method when a maximum number of iterations is reached.

Another disadvantage of the ``naive_Newton`` function is that it
calls the :math:`f(x)` function twice as many times as necessary. This extra
work is of no concern when :math:`f(x)` is fast to evaluate, but in
large-scale industrial software, one call to :math:`f(x)` might take hours or days, and then removing unnecessary calls is important. The solution
in our function is to store the call ``f(x)`` in a variable (``f_value``)
and reuse the value instead of making a new call ``f(x)``.

To summarize, we want to write an improved function for implementing
Newton's method where we

 * avoid division by zero

 * allow a maximum number of iterations

 * avoid the extra evaluation to :math:`f(x)`

A more robust and efficient version of the function, inserted in a
complete program
`Newtons_method.m <https://github.com/hplgit/prog4comp/src/m/Newtons_method.m>`__
for solving :math:`x^2 - 9 = 0`,
is listed below.

.. code-block:: matlab

        function Newtons_method()
            f = @(x) x^2 - 9;
            dfdx = @(x) 2*x;
            eps = 1e-6;
            x0 = 1000;
            [solution,no_iterations] = Newton(f, dfdx, x0, eps);
            if no_iterations > 0   % Solution found
                fprintf('Number of function calls: %d\n', 1 + 2*no_iterations);
                fprintf('A solution is: %f\n', solution)
            else
                fprintf('Abort execution.\n')
            end
        end
        
        function [solution, no_iterations] = Newton(f, dfdx, x0, eps)
            x = x0;
            f_value = f(x);
            iteration_counter = 0;
            while abs(f_value) > eps && iteration_counter < 100
                try
                    x = x - (f_value)/dfdx(x);
                catch
                    fprintf('Error! - derivative zero for x = \n', x)
                    exit(1)
                end
                f_value = f(x);
                iteration_counter = iteration_counter + 1;
            end
            % Here, either a solution is found, or too many iterations
            if abs(f_value) > eps
                iteration_counter = -1;
            end
            solution = x;
            no_iterations = iteration_counter;
        end

.. index::
   single: code; try-except

.. index::
   single: code; exception

Handling of the potential division by zero is done by a
``try-catch`` construction, which works as follows. First, Matlab tries to execute the code in the ``try`` block, but if something goes wrong there, the ``catch`` block is executed instead and the execution is terminated by ``exit``.

The division by zero will always be detected and the program will be
stopped. The main purpose of our way of treating the division by zero is
to give the user a more informative error message and stop the program
in a gentler way.

Calling
``exit``
with an argument different from zero (here ``1``)
signifies that the program stopped because of an error.
It is a good habit to supply the value ``1``, because tools in
the operating system can then be used by other programs to detect
that our program failed.

To prevent an infinite loop because of divergent iterations, we have
introduced the integer variable ``iteration_counter`` to count the
number of iterations in Newton's method.
With ``iteration_counter`` we can easily extend the condition in the
``while`` such that no more iterations take place when the number of
iterations reaches 100. We could easily let this limit be an argument
to the function rather than a fixed constant.

The ``Newton`` function returns the approximate solution and the number
of iterations. The latter equals :math:`-1` if the convergence criterion
:math:`|f(x)|<\epsilon` was not reached within the maximum number of
iterations. In the calling code, we print out the solution and
the number of function calls. The main cost of a method for
solving :math:`f(x)=0` equations is usually the evaluation of :math:`f(x)` and :math:`f'(x)`,
so the total number of calls to these functions is an interesting
measure of the computational work. Note that in function ``Newton``
there is an initial call to :math:`f(x)` and then one call to :math:`f` and one
to :math:`f'` in each iteration.

Running ``Newtons_method.m``, we get the following printout on the screen:

.. code-block:: text

        Number of function calls: 25
        A solution is: 3.000000

As we did with the integration methods in the chapter :ref:`sec:integrals`, we will
place our solvers for nonlinear algebraic equations in separate files for easy use by other programs. So, we place ``Newton`` in the file ``Newton.m``

.. index::
   single: Newton; starting value

The Newton scheme will work better if the starting value is close to
the solution. A good starting value may often make the difference as
to whether the code actually *finds* a solution or not.  Because of
its speed, Newton's method is often the method of first choice for
solving nonlinear algebraic equations, even if the scheme is not
guaranteed to work. In cases where the initial guess may be far from
the solution, a good strategy is to run a few iterations with the
bisection method (see the chapter :ref:`4th:NonlinAlgEq:Bisect`) to narrow
down the region where :math:`f` is close to zero and then switch to Newton's
method for fast convergence to the solution.

Newton's method requires the analytical expression for the
derivative :math:`f'(x)`. Derivation of :math:`f'(x)` is not always a reliable
process by hand if :math:`f(x)` is a complicated function.
However, Matlab has the Symbolic Math Toolbox, which we may use
to create the required ``dfdx`` function (Octave does not (yet) offer
the same possibilities for symbolic computations as Matlab. However,
there is work in progress, e.g. on using SymPy (from Python) from
Octave). In our sample problem, the recipe goes as follows:

.. code-block:: matlab

        syms x;                   % define x as a mathematical symbol
        f_expr = x^2 - 9;         % symbolic expression for f(x)
        dfdx_expr = diff(f_expr)  % compute f'(x) symbolically
        % Turn f_expr and dfdx_expr into plain Matlab functions
        f = matlabFunction(f_expr);
        dfdx = matlabFunction(dfdx_expr);
        dfdx(5)                   % will print 10

The nice feature of this code snippet is that ``dfdx_expr`` is the
exact analytical expression for the derivative, ``2*x``, if you
print it out. This is a symbolic expression so we cannot do
numerical computing with it, but the ``matlabFunction``
turns symbolic expressions into callable Matlab functions.

The next method is the secant method, which is usually slower than
Newton's method, but it does not require an expression for
:math:`f'(x)`, and it has only one function call per iteration.

.. _4th:NonlinAlgEq:Secant:

The secant method
=================

When finding the derivative :math:`f'(x)` in Newton's method is problematic,
or when function evaluations take too long; we may adjust the method
slightly. Instead of using tangent lines to the graph we may use
secants. The approach is referred to as the *secant method*, and the
idea is illustrated graphically in Figure :ref:`fig:illustrSecant` for
our example problem :math:`x^2 - 9 = 0`.

.. _fig:illustrSecant:

.. figure:: fig4_6.png
   :width: 400

   Illustrates the use of secants in the secant method when solving :math:`x^2 - 9 = 0, x \in [0, 1000]`. From two chosen starting values, :math:`x_0 = 1000` and :math:`x_1 = 700` the crossing :math:`x_2` of the corresponding secant with the :math:`x` axis is computed, followed by a similar computation of :math:`x_3` from :math:`x_1` and :math:`x_2`

The idea of the secant method is to think as in Newton's method, but
instead of using :math:`f'(x_n)`, we approximate this derivative by a
finite difference or the *secant*, i.e., the slope of the straight
line that goes through the two most recent approximations :math:`x_n` and
:math:`x_{n-1}`. This slope reads

.. _Eq:eFourTwelve:

.. math::
   :label: eFourTwelve
        
        \frac{f(x_n)-f(x_{n-1})}{x_n - x_{n-1}}\thinspace .
        
        

Inserting this expression for :math:`f'(x_n)` in Newton's method simply gives
us the secant method:

.. math::
        
        x_{n+1} = x_n - \frac{f(x_n)}{\frac{f(x_n)-f(x_{n-1})}{x_n - x_{n-1}}},

or

.. _Eq:eFourFourteen:

.. math::
   :label: eFourFourteen
        
        x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n)-f(x_{n-1})}
        \thinspace .
        
        

Comparing :eq:`eFourFourteen` to the graph in Figure
:ref:`fig:illustrSecant`, we see how *two* chosen starting points (:math:`x_0
= 1000`, :math:`x_1= 700`, and corresponding function values) are used to
compute :math:`x_2`. Once we have :math:`x_2`, we similarly use :math:`x_1` and :math:`x_2` to
compute :math:`x_3`. As with Newton's method, the procedure is repeated
until :math:`f(x_n)` is below some chosen limit value, or some limit on the
number of iterations has been reached. We use an iteration counter
here too, based on the same thinking as in the implementation of
Newton's method.

We can store the approximations :math:`x_n` in an array, but as in Newton's
method, we notice that the computation of :math:`x_{n+1}` only needs
knowledge of :math:`x_n` and :math:`x_{n-1}`, not "older"
approximations. Therefore, we can make use of only three variables:
``x`` for :math:`x_{n+1}`, ``x1`` for :math:`x_n`, and ``x0`` for :math:`x_{n-1}`. Note that
``x0`` and ``x1`` must be given (guessed) for the algorithm to start.

A program
`secant_method.m <https://github.com/hplgit/prog4comp/src/m/secant_method.m>`__
that solves our example problem may be written as:

.. code-block:: matlab

        function secant_method()
            f = @(x) x^2 - 9;
            eps = 1e-6;
            x0 = 1000;  x1 = x0 - 1;
            [solution,no_iterations] = secant(f, x0, x1, eps);
            if no_iterations > 0   % Solution found
                fprintf('Number of function calls: %d\n', 2 + no_iterations);
                fprintf('A solution is: %f\n', solution)
            else
                fprintf('Abort execution.\n')
            end
        end
        
        function [solution,no_iterations] = secant(f, x0, x1, eps)
            f_x0 = f(x0);
            f_x1 = f(x1);
            iteration_counter = 0;
            while abs(f_x1) > eps && iteration_counter < 100
                try
                    denominator = (f_x1 - f_x0)/(x1 - x0);
                    x = x1 - (f_x1)/denominator;
                catch
                    fprintf('Error! - denominator zero for x = \n', x1)
                    break
                end
                x0 = x1;
                x1 = x;
                f_x0 = f_x1;
                f_x1 = f(x1);
                iteration_counter = iteration_counter + 1;
            end
            % Here, either a solution is found, or too many iterations
            if abs(f_x1) > eps
                iteration_counter = -1;
            end
            solution = x1;
            no_iterations = iteration_counter;
        end

The number of function calls is now related to ``no_iterations``,
i.e., the number of iterations, as ``2 + no_iterations``, since we need
two function calls before entering the ``while`` loop, and then one
function call per loop iteration. Note that, even though we need two
points on the graph to compute each updated estimate, only a *single*
function call (``f(x1)``) is required in each iteration since
``f(x0)`` becomes the "old" ``f(x1)`` and may simply be copied as
``f_x0 = f_x1`` (the exception is the very first iteration where two function
evaluations are needed).

Running ``secant_method.m``, gives the following printout on the screen:

.. code-block:: text

        Number of function calls: 19
        A solution is: 3.000000

As with the function ``Newton``, we place ``secant`` in
a separate file ``secant.m`` for easy use later.

.. _4th:NonlinAlgEq:Bisect:

The bisection method
====================

Neither Newton's method nor the secant method can guarantee that an
existing solution will be found (see :ref:`4th:exer:Newton:failure` and :ref:`4th:exer:Secant:failure`). The
bisection method, however, does that. However, if there are several
solutions present, it finds only one of them, just as Newton's method
and the secant method.  The bisection method is slower than the other
two methods, so reliability comes with a cost of speed.

To solve :math:`x^2 - 9 = 0`, :math:`x \in \left[0, 1000\right]` with the
bisection method, we reason as follows.  The first key idea is that if :math:`f(x)
= x^2 - 9` is *continuous* on the interval and the function values for
the interval endpoints (:math:`x_L = 0`, :math:`x_R =1000`) have *opposite signs*,
:math:`f(x)` *must* cross the :math:`x` axis at least once on the interval. That
is, we know there is at least one solution.

The second key idea comes from dividing the interval in two equal
parts, one to the left and one to the right of the midpoint :math:`x_M =
500`. By evaluating the sign of :math:`f(x_M)`, we will immediately know
whether a solution must exist to the left or right of :math:`x_M`. This is
so, since if :math:`f(x_M) \ge 0`, we know that :math:`f(x)` has to cross the :math:`x`
axis between :math:`x_L` and :math:`x_M` at least once (using the same
argument as for the original interval). Likewise, if instead :math:`f(x_M)
\le 0`, we know that :math:`f(x)` has to cross the :math:`x` axis between :math:`x_M`
and :math:`x_R` at least once.

In any case, we may proceed with half the interval only. The exception
is if :math:`f(x_M) \approx 0`, in which case a solution is found. Such
interval halving can be continued until a solution is found. A
"solution" in this case, is when :math:`|f(x_M)|` is sufficiently close to
zero, more precisely (as before): :math:`|f(x_M)|<\epsilon`, where
:math:`\epsilon` is a small number specified by the user.

The sketched strategy seems reasonable, so let us write a
reusable function that can
solve a general algebraic equation :math:`f(x)=0`
(`bisection_method.m <https://github.com/hplgit/prog4comp/src/m/bisection_method.m>`__):

.. code-block:: matlab

        function bisection_method()
            f = @(x) x^2 - 9;
            eps = 1e-6;
            a = 0;   b = 1000;
            [solution, no_iterations] = bisection(f, a, b, eps);
            if solution <= b   % Solution found
                fprintf('Number of function calls: %d\n', 1+2*no_iterations);
                fprintf('A solution is: %f\n', solution);
            else
                fprintf('Abort execution.\n');
            end
        end
        
        function [result1, result2] = bisection(f, x_L, x_R, eps)
            if f(x_L)*f(x_R) > 0
                fprintf('Error! Function does not have opposite\n');
                fprintf('signs at interval endpoints!')
                exit(1)
            end
            x_M = (x_L + x_R)/2.0;
            f_M = f(x_M);
            iteration_counter = 1;
            while abs(f_M) > eps
                left_f = f(x_L);
                right_f = f(x_R);
                if left_f*f_M > 0   % i.e., same sign
                    x_L = x_M;
                else
                    x_R = x_M;
                end
                x_M = (x_L + x_R)/2;
                f_M = f(x_M);
                iteration_counter = iteration_counter + 2;
            end
            result1 = x_M;
            result2 = iteration_counter;
        end

.. index::
   single: return; None

.. index:: sys.exit

Note that we first check if :math:`f` changes sign in :math:`[a,b]`, because that
is a requirement for the algorithm to work. The algorithm also relies
on a continuous :math:`f(x)` function, but this is very challenging
for a computer code to check.

We get the following printout to the screen when ``bisection_method.m`` is run:

.. code-block:: text

        Number of function calls: 61
        A solution is: 3.000000

We notice that
the number of function calls is much higher than with the previous methods.


.. admonition:: Required work in the bisection method

   If the starting interval of the bisection method is bounded by :math:`a` and
   :math:`b`, and the solution at step :math:`n` is taken to be the middle value, the
   error is bounded as
   
   .. _Eq:eFourSeven:

.. math::
   :label: eFourSeven
           
           \frac{|b-a|}{2^n},
           
           
   
   because the initial interval has been halved :math:`n` times.
   Therefore, to meet a tolerance :math:`\epsilon`, we need :math:`n` iterations such that
   the length of the current interval equals :math:`\epsilon`:
   
   .. math::
            \frac{|b-a|}{2^n}=\epsilon\quad\Rightarrow\quad
           n = \frac{\ln ((b-a)/\epsilon)}{\ln 2}\thinspace .
           
   
   This is a great advantage of the bisection method: we know beforehand
   how many iterations :math:`n` it takes to meet a certain accuracy
   :math:`\epsilon` in the solution.




As with the two previous methods, the function ``bisection`` is
stored as a separate file ``bisection.m`` for easy use by other programs.

.. _4th:NonlinAlgEq:RateConv:

Rate of convergence
===================

With the methods above, we noticed that the number of iterations or
function calls could differ quite substantially.
The number of iterations needed
to find a solution is closely related to the *rate of convergence*,
which is the speed of the error as we approach the root.
More precisely, we introduce the error in iteration :math:`n` as
:math:`e_n=|x-x_n|`, and define the *convergence rate* :math:`q` as

.. _Eq:eFourFifteen:

.. math::
   :label: eFourFifteen
        
        e_{n+1} = Ce_n^q,
        
        

where :math:`C` is a constant. The exponent :math:`q` measures how fast the error
is reduced from one iteration to the next. The larger :math:`q` is,
the faster the error goes to zero, and the fewer iterations we
need to meet the stopping criterion :math:`|f(x)|<\epsilon`.

A single :math:`q` in :eq:`eFourFifteen` is defined in the limit
:math:`n\rightarrow\infty`. For finite :math:`n`, and especially smaller :math:`n`,
:math:`q` will vary with :math:`n`. To estimate :math:`q`, we can compute
all the errors :math:`e_n` and set up
:eq:`eFourFifteen` for three consecutive experiments :math:`n-1`, :math:`n`, and
:math:`n+1`:

.. math::
        
        e_{n} &= Ce_{n-1}^q,\\ 
        e_{n+1} &= Ce_n^q\thinspace .
        

Dividing these two equations by each other and solving with respect to
:math:`q` gives

.. math::
         q = \frac{\ln (e_{n+1}/e_n)}{\ln(e_n/e_{n-1})}\thinspace .

Since this :math:`q` will vary somewhat with :math:`n`, we call it :math:`q_n`.
As :math:`n` grows, we expect :math:`q_n` to approach a limit (:math:`q_n\rightarrow q`).
To compute all the :math:`q_n` values, we need all the :math:`x_n` approximations.
However, our previous implementations of Newton's method, the secant
method, and the bisection method returned just the final approximation.

Therefore, we have extended those previous implementations such that
the user can choose whether the final value or the whole history of solutions is
to be returned. The extended implementations are named ``Newton_solver``, ``secant_solver``
and ``bisection_solver``. Compared to the previous implementations, each of these now takes
an extra parameter ``return_x_list``. This parameter is a boolean, set to ``true`` if the
function is supposed to return all the root approximations, or ``false``, if the function
should only return the final approximation. As an example, let us take a closer look
at ``Newton_solver``:

.. code-block:: matlab

        function [sol, no_it] = Newton_solver(f, dfdx, x, eps, return_x_list)
            f_value = f(x);
            iteration_counter = 0;
            if return_x_list
                x_list = [];
            end
            while abs(f_value) > eps && iteration_counter < 100
                try
                    x = x - (f_value)/dfdx(x);
                catch
                    fprintf('Error! - derivative zero for x = \n', x)
                    break
                end
                f_value = f(x);
                iteration_counter = iteration_counter + 1;
                if return_x_list
                    x_list = [x_list x];
                end
            end
            % Here, either a solution is found, or too many iterations
            if abs(f_value) > eps
                iteration_counter = -1;   % i.e., lack of convergence
            end
            
            if return_x_list
                sol = x_list;
                no_it = iteration_counter;
            else
                sol = x;
                no_it = iteration_counter;
            end
        end

The function is found in the file
`Newton_solver.m <https://github.com/hplgit/prog4comp/src/m/Newton_solver.m>`__.

We can now make a call

.. code-block:: matlab

        [x, iter] = Newton_solver(f, dfdx, 1000, 1e-6, true);

and get an array ``x`` returned. With knowledge of the exact
solution :math:`x` of :math:`f(x)=0`, we can compute all the errors :math:`e_n`
and associated :math:`q_n` values with the compact function

.. code-block:: matlab

        function q = rate(x, x_exact)
            e = abs(x - x_exact);
            q = zeros(length(e)-2,1);
            for n = 2:(length(e)-1)
                q(n-1) = log(e(n+1)/e(n))/log(e(n)/e(n-1));       
            end
        end

The error model :eq:`eFourFifteen` works well for Newton's method and
the secant method. For the bisection method, however, it works well
in the beginning, but not when the solution is approached.

We can compute the rates :math:`q_n` and print them nicely,

.. code-block:: matlab

        function print_rates(method, x, x_exact)
            q = rate(x, x_exact);
            fprintf('%s:\n', method)
            for i = 1:length(q)
                fprintf('%.2f ', q(i));
            end
            fprintf('\n')
        end

The result for ``print_rates('Newton', x, 3)`` is

.. code-block:: text

        Newton:
        1.01 1.02 1.03 1.07 1.14 1.27 1.51 1.80 1.97 2.00

indicating that :math:`q=2` is the rate for Newton's method.
A similar computation using the secant method, gives the rates

.. code-block:: text

        secant:
        1.26 0.93 1.05 1.01 1.04 1.05 1.08 1.13 1.20 1.30 1.43
        1.54 1.60 1.62 1.62

Here it seems that :math:`q\approx 1.6` is the limit.

**Remark.**
If we in the bisection method think of the length of the current
interval containing the solution as the error :math:`e_n`, then
:eq:`eFourFifteen` works perfectly since :math:`e_{n+1}=\frac{1}{2}e_n`,
i.e., :math:`q=1` and :math:`C=\frac{1}{2}`, but if :math:`e_n` is the true error
:math:`|x-x_n|`, it is easily seen from a sketch that this error can
oscillate between the current interval length and a potentially very small
value as we approach the exact solution. The corresponding rates :math:`q_n`
fluctuate widely and are of no interest.

Exercises  (6)
==============

.. --- begin exercise ---

.. _4th:exer:Newton:failure:

Exercise 72: Understand why Newton's method can fail
----------------------------------------------------

The purpose of this exercise is to understand when Newton's method works
and fails. To this end, solve :math:`\tanh x=0` by Newton's method
and study the intermediate details of the algorithm. Start with :math:`x_0=1.08`.
Plot the tangent in each iteration of Newton's method. Then repeat the calculations
and the plotting when :math:`x_0=1.09`. Explain what you observe.

.. --- begin solution of exercise ---

**Solution.**
The program may be written as:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: Newton_failure


which may be called from the script ``use_my_Newton_failure.m``:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: use_my_Newton_failure


Note that ``Newton_failure.m`` calls the function ``plot_line`` (located in ``plot_line.m``),
reading:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: plot_line


(The function ``plot_line`` is placed as a separate m-file so that it may be used also
by the function ``secant_failure``, which is to be written in another exercise.)

Running the program with ``x`` set to :math:`1.08` produces a series of plots (and prints) showing
the graph and the tangent for the present value of ``x``. There are quite many
plots, so we do not show them here. However, the tangent line "jumps" around
a few times before it settles. In the final plot the tangent line goes through
the solution at :math:`x = 0`. The final printout brings the information:

.. code-block:: text

        Number of function calls: 13
        A solution is: 0.000024

When we run the program anew, this time with ``x`` set to :math:`1.09`, we get another series of
plots (and prints), but this time the tangent moves away from the (known) solution.
The final printout we get states that:

.. code-block:: text

        Number of function calls: 19
        A solution is: nan

Here, ``nan`` stands for "not a number", meaning that we got no solution value for ``x``.
That is, Newton's method diverged.

.. --- end solution of exercise ---

Filename: ``Newton_failure.*``.

.. --- end exercise ---

.. --- begin exercise ---

.. _4th:exer:Secant:failure:

Exercise 73: See if the secant method fails
-------------------------------------------

Does the secant method behave better than Newton's method in
the problem described in :ref:`4th:exer:Newton:failure`?
Try the initial guesses

1. :math:`x_0=1.08` and :math:`x_1=1.09`

2. :math:`x_0=1.09` and :math:`x_1=1.1`

3. :math:`x_0=1` and :math:`x_1=2.3`

4. :math:`x_0=1` and :math:`x_1=2.4`

.. --- begin solution of exercise ---

**Solution.**
The program may be written as:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: secant_failure


which may be called from the script ``use_my_secant_failure.m``:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: use_my_secant_failure


Note that, as with ``Newton_failure.m``, the script ``plot_line.m`` is called for
plotting each tangent.

The script converges with the three first-mentioned alternatives for :math:`x_0` and :math:`x_1`.
With the final set of parameter values, the method diverges with a printout:

.. code-block:: text

        Error! - denominator zero for x = 360.600893792

and a few more lines stating that an exception error has occurred.

.. --- end solution of exercise ---

Filename: ``secant_failure.*``.

.. --- end exercise ---

.. --- begin exercise ---

.. _4th:exer:Bisection:failure:

Exercise 74: Understand how the bisection method cannot fail
------------------------------------------------------------

Solve the same problem as in :ref:`4th:exer:Newton:failure`,
using the bisection method, but let the initial interval be
:math:`[-5,3]`. Report how the interval containing the solution evolves
during the iterations.

.. --- begin solution of exercise ---

**Solution.**
The code may be written as:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: bisection_nonfailure


which may be called from the script ``use_my_bisection_nonfailure.m``:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: use_my_bisection_nonfailure


Running the program produces the following printout:

.. code-block:: text

        interval: [-1.000000, 3.000000]
        interval: [-1.000000, 1.000000]
        Number of function calls: 7
        A solution is: 0.000000

.. --- end solution of exercise ---

Filename: ``bisection_nonfailure.*``.

.. --- end exercise ---

.. --- begin exercise ---

.. _4th:exer:Bisection:Newton:

Exercise 75: Combine the bisection method with Newton's method
--------------------------------------------------------------

.. solution=`sol_bisection_Newton.pdf`

An attractive idea is to combine the reliability of the bisection method
with the speed of Newton's method, even if the potential divergence with
Newton's method then still is an issue. Such a combination is implemented
by running the bisection method until we have a narrow interval, and then switch
to Newton's method for speed.

Write a function that implements this idea.
Start with an interval :math:`[a,b]` and switch to Newton's method when
the current interval in the bisection method is a fraction :math:`s`
of the initial interval (i.e., when the interval has length :math:`s(b-a)`).
The value of :math:`s` must be given as an argument to the function, but
it may have a default value of 0.1.

Try the new method on :math:`\tanh(x)=0` with an initial interval :math:`[-10,15]`.

.. --- begin solution of exercise ---

**Solution.**
The code may be written as:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: bisection_Newton


which may be called from the script ``use_my_bisection_Newton.m``:


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: use_my_bisection_Newton


Running the program produces the following printout:

.. code-block:: text

        A solution x = 0.000000 was reached in 7 iterations

.. --- end solution of exercise ---

Filename: ``bisection_Newton.m``.

.. --- end exercise ---

.. --- begin exercise ---

.. _4th:exer:NewtonHand:

Exercise 76: Write a test function for Newton's method
------------------------------------------------------

The purpose of this function is to verify the implementation of Newton's
method in the ``Newton`` function in the file ``nonlinear_solvers.py``.
Construct an algebraic equation and perform two iterations of Newton's
method by
hand.
Find the corresponding size of :math:`|f(x)|` and use this
as value for ``eps`` when calling ``Newton``. The function should then
also perform two iterations and return the same approximation to
the root as you calculated manually. Implement this idea for a unit test
as a test function ``test_Newton()``.

.. --- begin solution of exercise ---

**Solution.**
Here is the complete module with the test function.


.. The below box could be typeset as .. admonition: Attention
   but we have decided not to do so since the admon needs a title
   (the box formatting is therefore just ignored)

Here is a missing file: test_Newton


.. --- end solution of exercise ---

Filename: ``test_Newton.m``.

.. --- end exercise ---

.. --- begin exercise ---

.. _4th:exer:beam:vib:

Exercise 77: Solve nonlinear equation for a vibrating beam
----------------------------------------------------------

An important engineering problem that arises in a lot of applications
is the vibrations of a clamped beam where the other end is free.
This problem can be analyzed
analytically, but the calculations boil down to solving the
following nonlinear algebraic equation:

.. math::
         \cosh\beta \cos\beta = -1,

where :math:`\beta` is related to important beam parameters through

.. math::
         \beta^4 = \omega^2 \frac{\varrho A}{EI},

where :math:`\varrho` is the density of the beam, :math:`A` is the area of
the cross section, :math:`E` is Young's modulus,
and :math:`I` is the moment of the inertia of the cross section. The most
important parameter of interest is :math:`\omega`, which is the frequency
of the beam. We want to compute the frequencies of a
vibrating steel beam with a rectangular cross section having
width :math:`b=25` mm and height :math:`h=8` mm.
The density of steel is :math:`7850 \mbox{ kg/m}^3`,
and :math:`E= 2\cdot 10^{11}` Pa. The moment of inertia of a rectangular cross section
is :math:`I=bh^3/12`.

**a)**
Plot the equation to be solved so that one can inspect where the zero
crossings occur.

.. --- begin hint in exercise ---

**Hint.**
When writing the equation as :math:`f(\beta)=0`, the :math:`f` function increases
its amplitude dramatically with :math:`\beta`. It is therefore wise to
look at an equation with damped amplitude, :math:`g(\beta) = e^{-\beta}f(\beta) = 0`.
Plot :math:`g` instead.

.. --- end hint in exercise ---

.. --- begin solution of exercise ---

**Solution.**

.. figure:: beam_vib_expmxf.png
   :width: 600

.. --- end solution of exercise ---

**b)**
Compute the first three frequencies.

.. --- begin solution of exercise ---

**Solution.**
Here is a complete program, using the Bisection method for root
finding, based on intervals found from the plot above.

.. code-block:: matlab

        function beam_vib()
        
            plot_f()
        
            f_handle = @f;
            % Set up suitable intervals
            intervals = [1 3; 4 6; 7 9];
            betas = [];  % roots
            for i = 1:length(intervals) 
                beta_L = intervals(i, 1);  beta_R = intervals(i, 2);
                [beta, it] = bisection(f_handle, beta_L, beta_R, eps=1E-6);
                betas = [betas beta];
                f(beta)   
            end
            betas
        
            % Find corresponding frequencies
        
            function value = omega(beta, rho, A, E, I)
                value = sqrt(beta^4/(rho*A/(E*I)));
            end
        
            rho = 7850;  % kg/m^3
            E = 1.0E+11; % Pa
            b = 0.025;   % m
            h = 0.008;   % m
            A = b*h;
            I = b*h^3/12;
        
            for i = 1:length(betas)
                omega(betas(i), rho, A, E, I)
            end
        
        end
        
        function value = f(beta)
            value = cosh(beta).*cos(beta) + 1;
        end
        
        function value = damped(beta)
            % Damp the amplitude of f. It grows like cosh, i.e. exp.
            value = exp(-beta).*f(beta);
        end
        
        function plot_f()
            beta = linspace(0, 20, 501);
            %y = f(x)
            y = damped(beta);
            plot(beta, y, 'r', [beta(1), beta(length(beta))], [1, 1], 'b--')
            grid('on');
            xlabel('beta');
            ylabel('exp(-beta) (cosh(beta) cos(beta) +1)')
            savefig('tmp1.png'); savefig('tmp1.pdf')
        end

The output of :math:`\beta` reads :math:`1.875`, :math:`4.494`, :math:`7.855`, and corresponding
:math:`\omega` values are :math:`29`, :math:`182`, and :math:`509` Hz.

.. --- end solution of exercise ---

Filename: ``beam_vib.m``.

.. --- end exercise ---

